"""
Sistema de transcripci√≥n y diarizaci√≥n de audio con enfoque funcional.
Usa tipado est√°tico espec√≠fico y decoradores para importaciones din√°micas.
"""
from __future__ import annotations
import os
os.environ["TRANSFORMERS_VERBOSITY"] = "error"

import time
import logging
import functools
import json
import sys
import argparse
import importlib
import requests
from pathlib import Path
from typing import TypedDict, Dict, List, Any, Union, Optional, Callable, Tuple, cast, Literal
from rich.progress import Progress, TextColumn, BarColumn, TimeElapsedColumn

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# Configuraci√≥n de logging con formato compacto
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s-%(name)s-%(levelname)s - %(message)s',
                    datefmt='%y%m%d.%H%M%S')
logger = logging.getLogger("transcriber")

# Tipos espec√≠ficos para datos
TaskType = Literal["transcribe", "translate"]

class AudioData(TypedDict):
    """Datos de audio procesados y listos para transcripci√≥n/diarizaci√≥n."""
    numpy_array: "np.ndarray"  # Array de audio mono de 16kHz
    torch_tensor: "torch.Tensor"  # Tensor para diarizaci√≥n
    sampling_rate: int  # Siempre 16000Hz

class TranscriptChunk(TypedDict):
    """Fragmento individual de transcripci√≥n con timestamp."""
    text: str
    timestamp: Tuple[Optional[float], Optional[float]]

class TranscriptOutput(TypedDict):
    """Resultado de la transcripci√≥n completa."""
    text: str  # Texto completo
    chunks: List[TranscriptChunk]  # Fragmentos con timestamps

class SpeakerSegment(TypedDict):
    """Segmento con informaci√≥n de hablante."""
    segment: Dict[str, float]  # start, end en segundos
    speaker: str  # Identificador del hablante (SPEAKER_00, etc.)

class DiarizedChunk(TranscriptChunk):
    """Fragmento de transcripci√≥n con informaci√≥n de hablante."""
    speaker: str  # Identificador del hablante

class FinalResult(TypedDict):
    """Estructura final del resultado."""
    speakers: List[DiarizedChunk]  # Transcripci√≥n con hablantes
    chunks: List[TranscriptChunk]  # Fragmentos originales
    text: str  # Texto completo

# Configuraci√≥n usando dataclass congelado
from dataclasses import dataclass

@dataclass(frozen=True)
class TranscriptionConfig:
    """Configuraci√≥n inmutable para el proceso de transcripci√≥n."""
    file_name: Union[str, Path]
    device_id: str = "0"
    transcript_path: Union[str, Path] = "output.json"
    model_name: str = "openai/whisper-large-v3"
    task: TaskType = "transcribe"
    language: str = "es"
    batch_size: int = 8
    hf_token: str = "no_token"
    diarization_model: str = "pyannote/speaker-diarization-3.1"
    num_speakers: Optional[int] = None
    min_speakers: Optional[int] = None
    max_speakers: Optional[int] = None
    
    def __post_init__(self) -> None:
        """Validar configuraci√≥n despu√©s de inicializaci√≥n."""
        if self.num_speakers is not None and (
            self.min_speakers is not None or self.max_speakers is not None
        ):
            raise ValueError(
                "--num-speakers no puede usarse junto con --min-speakers o --max-speakers"
            )

# Utilidad para importaci√≥n din√°mica
def import_module(module_name: str) -> Any:
    """Importa un m√≥dulo din√°micamente y registra en el log."""
    logger.debug(f"Importando: {module_name}")
    try:
        return importlib.import_module(module_name)
    except ImportError as e:
        logger.error(f"Error importando {module_name}: {e}")
        raise

# Decorador para importar m√≥dulos justo antes de ejecutar la funci√≥n
def with_imports(*module_names: str) -> Callable:
    """
    Decorador que importa m√≥dulos justo antes de ejecutar la funci√≥n.
    
    Args:
        *module_names: Nombres de los m√≥dulos a importar
    
    Returns:
        Funci√≥n decorada que tendr√° los m√≥dulos importados disponibles
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            dynamic_imports: Dict[str, Any] = {}
            for name in module_names:
                module_alias = name.split(".")[-1]
                dynamic_imports[module_alias] = import_module(name)
            kwargs["dynamic_imports"] = dynamic_imports
            return func(*args, **kwargs)
        return wrapper
    return decorator

# Decorador para medir tiempo de ejecuci√≥n
def log_time(func: Callable) -> Callable:
    """Decorador que mide el tiempo de ejecuci√≥n de una funci√≥n."""
    @functools.wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> Any:
        start_time: float = time.time()
        result: Any = func(*args, **kwargs)
        end_time: float = time.time()
        logger.info(f"{func.__name__} - Tiempo: {end_time - start_time:.2f}s")
        return result
    return wrapper

# Funci√≥n auxiliar para mostrar barra de progreso
def with_progress_bar(description: str, func: Callable) -> Any:
    """
    Ejecuta una funci√≥n mostrando una barra de progreso.
    
    Args:
        description: Descripci√≥n para la barra de progreso
        func: Funci√≥n a ejecutar
        
    Returns:
        Resultado de la funci√≥n
    """
    with Progress(
        TextColumn("ü§ó [progress.description]{task.description}"),
        BarColumn(style="yellow1", pulse_style="white"),
        TimeElapsedColumn(),
    ) as progress:
        task_id = progress.add_task(f"[yellow]{description}", total=None)
        return func()

@log_time
def parse_arguments() -> TranscriptionConfig:
    """
    Parsea los argumentos de l√≠nea de comandos y devuelve configuraci√≥n tipada.
    
    Returns:
        TranscriptionConfig: Configuraci√≥n inmutable con valores validados
    """
    parser: argparse.ArgumentParser = argparse.ArgumentParser(description="Transcripci√≥n de audio")
    parser.add_argument(
        "file_name", type=str, help="Ruta o URL al archivo de audio"
    )
    parser.add_argument(
        "--device-id", default="0", type=str, help="ID del dispositivo GPU"
    )
    parser.add_argument(
        "--transcript-path", default="output.json", type=str, help="Ruta de salida"
    )
    parser.add_argument(
        "--model-name", default="openai/whisper-large-v3", type=str
    )
    parser.add_argument(
        "--task", default="transcribe", choices=["transcribe", "translate"], 
        type=str, help="Tarea a realizar"
    )
    parser.add_argument(
        "--language", default="es", type=str, help="Idioma del audio"
    )
    parser.add_argument(
        "--batch-size", default=8, type=int, help="Tama√±o de lote"
    )
    parser.add_argument(
        "--hf-token", default="no_token", type=str, help="Token de HuggingFace"
    )
    parser.add_argument(
        "--diarization-model", default="pyannote/speaker-diarization-3.1", type=str
    )
    parser.add_argument(
        "--num-speakers", default=None, type=int, help="N√∫mero exacto de hablantes"
    )
    parser.add_argument(
        "--min-speakers", default=None, type=int, help="N√∫mero m√≠nimo de hablantes"
    )
    parser.add_argument(
        "--max-speakers", default=None, type=int, help="N√∫mero m√°ximo de hablantes"
    )

    args: argparse.Namespace = parser.parse_args()
    
    # Crear configuraci√≥n tipada y validada
    return TranscriptionConfig(
        file_name=args.file_name,
        device_id=args.device_id,
        transcript_path=args.transcript_path,
        model_name=args.model_name,
        task=cast(TaskType, args.task),  # Cast para Literal
        language=args.language,
        batch_size=args.batch_size,
        hf_token=args.hf_token,
        diarization_model=args.diarization_model,
        num_speakers=args.num_speakers,
        min_speakers=args.min_speakers,
        max_speakers=args.max_speakers
    )

@log_time
@with_imports("numpy", "torch", "transformers.pipelines.audio_utils", "torchaudio.functional", "os", "subprocess")
def process_audio(
    input_src: Union[str, Path, bytes, Dict[str, Any]],
    *,
    dynamic_imports: Dict[str, Any] = {}
) -> AudioData:
    """
    Procesa el audio desde varias fuentes para ASR y diarizaci√≥n.
    Detecta autom√°ticamente archivos de video y extrae el audio.
    
    Args:
        input_src: String (ruta/URL), bytes o diccionario con datos de audio.
        dynamic_imports: M√≥dulos importados din√°micamente.
        
    Returns:
        AudioData: Diccionario con numpy_array, torch_tensor y sampling_rate.
    """
    logger.info(f"Procesando audio desde {type(input_src)}")
    
    # Extraer m√≥dulos
    np = dynamic_imports["numpy"]
    torch = dynamic_imports["torch"]
    audio_utils = dynamic_imports["audio_utils"]
    functional = dynamic_imports["functional"]
    os = dynamic_imports["os"]
    subprocess = dynamic_imports["subprocess"]

    # Procesamiento seg√∫n tipo de entrada
    if isinstance(input_src, (str, Path)):
        input_str: str = str(input_src)
        _, file_ext = os.path.splitext(input_str.lower())
        is_video = file_ext in ['.mp4', '.avi', '.mov', '.mkv', '.webm']
        
        if input_str.startswith(("http://", "https://")):
            logger.info("Descargando audio desde URL")
            input_src = requests.get(input_str).content
        else:
            logger.info("Cargando archivo desde local")
            if is_video:
                logger.info(f"Detectado archivo de video: {file_ext}")
                temp_audio = f"{os.path.splitext(input_str)[0]}_temp.wav"
                logger.info(f"Extrayendo audio a archivo temporal: {temp_audio}")
                try:
                    subprocess.run(
                        ["ffmpeg", "-y", "-i", input_str, "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1", temp_audio],
                        check=True,
                        capture_output=True
                    )
                    # Verificar que el archivo se gener√≥ correctamente
                    if not os.path.exists(temp_audio) or os.path.getsize(temp_audio) == 0:
                        raise ValueError("El archivo temporal de audio no se gener√≥ correctamente o est√° vac√≠o")
                    with open(temp_audio, "rb") as f:
                        input_src = f.read()
                    os.remove(temp_audio)
                except subprocess.CalledProcessError as e:
                    logger.error(f"Error al extraer audio con FFmpeg: {e}")
                    logger.error(f"Salida de error: {e.stderr.decode() if e.stderr else 'No stderr'}")
                    raise ValueError(f"No se pudo extraer audio del archivo de video: {input_str}") from e
                except FileNotFoundError:
                    logger.error("FFmpeg no est√° instalado o no se encuentra en el PATH")
                    raise ValueError("FFmpeg es necesario para procesar archivos de video")
            else:
                with open(input_str, "rb") as f:
                    input_src = f.read()

    if isinstance(input_src, bytes):
        logger.info("Decodificando audio con FFmpeg")
        try:
            input_src = audio_utils.ffmpeg_read(input_src, 16000)
        except Exception as e:
            logger.error("Error al decodificar el audio con FFmpeg", exc_info=True)
            raise ValueError("No se pudo decodificar el audio correctamente. Verifica el formato del archivo.") from e

    if isinstance(input_src, dict):
        logger.info("Procesando entrada tipo diccionario")
        if not ("sampling_rate" in input_src and ("raw" in input_src or "array" in input_src)):
            raise ValueError("El diccionario debe contener 'raw' o 'array' con el audio y 'sampling_rate'")
        _inputs: Optional[Any] = input_src.pop("raw", None)
        if _inputs is None:
            input_src.pop("path", None)
            _inputs = input_src.pop("array", None)
        in_sampling_rate: int = input_src.pop("sampling_rate")
        input_src = _inputs
        if in_sampling_rate != 16000:
            logger.info(f"Remuestreando de {in_sampling_rate} a 16000 Hz")
            input_src = functional.resample(torch.from_numpy(input_src), in_sampling_rate, 16000).numpy()

    if not isinstance(input_src, np.ndarray):
        raise ValueError(f"Se esperaba un array numpy, se obtuvo `{type(input_src)}`")
    if len(input_src.shape) != 1:
        raise ValueError("Se esperaba audio de un solo canal")

    # Preparar tensor para la diarizaci√≥n
    input_src = input_src.copy() 
    torch_tensor = torch.from_numpy(input_src).float().unsqueeze(0)
    
    logger.info(f"Audio procesado: forma={input_src.shape}, SR=16000Hz")
    return {
        "numpy_array": input_src,
        "torch_tensor": torch_tensor,
        "sampling_rate": 16000
    }


@log_time
@with_imports("torch", "transformers")
def transcribe_audio(
    config: TranscriptionConfig, 
    audio_data: AudioData,
    *, 
    dynamic_imports: Dict[str, Any] = {}
) -> TranscriptOutput:
    """
    Transcribe el audio usando el modelo Whisper.

    Args:
        config: Configuraci√≥n de transcripci√≥n.
        audio_data: Audio procesado, conteniendo "numpy_array" y "sampling_rate".
        dynamic_imports: M√≥dulos importados din√°micamente.

    Returns:
        TranscriptOutput: Resultado de la transcripci√≥n.
    """
    logger.info(f"Iniciando transcripci√≥n con modelo {config.model_name}")
    
    torch = dynamic_imports["torch"]
    transformers = dynamic_imports["transformers"]
    
    # Configurar el pipeline de ASR
    pipe = transformers.pipeline(
        "automatic-speech-recognition",
        model=config.model_name,
        torch_dtype=torch.float16,
        device="cuda:" + config.device_id if config.device_id != "mps" else "mps",
        model_kwargs={"attn_implementation": "sdpa"},
    )
    
    # Par√°metros de generaci√≥n
    generate_kwargs: Dict[str, str] = {
        "task": config.task,
        "language": config.language,
    }
    
    def execute_transcription() -> Any:
        # Construir el diccionario de entrada seg√∫n lo que espera el pipeline:
        # Debe contener una clave "raw" con el array de audio y "sampling_rate"
        audio_dict = {
            "raw": audio_data["numpy_array"].copy(),  # Hacemos copy() para que sea escribible
            "sampling_rate": audio_data["sampling_rate"],
        }
        
        return pipe(
            audio_dict,
            chunk_length_s=30,
            batch_size=config.batch_size,
            generate_kwargs=generate_kwargs,
            return_timestamps=True,
        )
    
    outputs: Any = with_progress_bar("Transcribiendo...", execute_transcription)
    logger.info(f"Transcripci√≥n completa: {len(outputs.get('chunks', []))} fragmentos")
    return cast(TranscriptOutput, outputs)




@with_imports("torch", "pyannote.audio")
def load_diarization_pipeline(
    config: TranscriptionConfig,
    *, 
    dynamic_imports: Dict[str, Any] = {}
) -> Any:
    """
    Carga el pipeline de diarizaci√≥n.
    
    Args:
        config: Configuraci√≥n de transcripci√≥n/diarizaci√≥n
        dynamic_imports: M√≥dulos importados din√°micamente
        
    Returns:
        Pipeline: Pipeline de diarizaci√≥n cargado
    """
    torch = dynamic_imports["torch"]
    pyannote_audio = dynamic_imports["pyannote.audio"]
    
    pipeline = pyannote_audio.Pipeline.from_pretrained(
        checkpoint_path=config.diarization_model,
        use_auth_token=config.hf_token,
    )
    
    device = torch.device("mps" if config.device_id == "mps" else f"cuda:{config.device_id}")
    pipeline.to(device)
    
    return pipeline

def process_diarization_segments(diarization: Any) -> List[Dict[str, Any]]:
    """
    Procesa los segmentos de diarizaci√≥n y los combina por hablante.
    
    Args:
        diarization: Resultado de la diarizaci√≥n
        
    Returns:
        List[Dict[str, Any]]: Segmentos combinados por hablante
    """
    # Extraer segmentos
    segments: List[Dict[str, Any]] = []
    for segment, track, label in diarization.itertracks(yield_label=True):
        segments.append({
            "segment": {"start": segment.start, "end": segment.end},
            "track": track,
            "label": label,
        })
    
    # Combinar segmentos consecutivos del mismo hablante
    new_segments: List[Dict[str, Any]] = []
    if segments:
        prev_segment: Dict[str, Any] = segments[0]
        
        for i in range(1, len(segments)):
            cur_segment: Dict[str, Any] = segments[i]
            
            # Si cambi√≥ el hablante, agregar el segmento combinado
            if cur_segment["label"] != prev_segment["label"]:
                new_segments.append({
                    "segment": {
                        "start": prev_segment["segment"]["start"],
                        "end": cur_segment["segment"]["start"],
                    },
                    "speaker": prev_segment["label"],
                })
                prev_segment = segments[i]
        
        # Agregar el √∫ltimo segmento
        new_segments.append({
            "segment": {
                "start": prev_segment["segment"]["start"],
                "end": segments[-1]["segment"]["end"],
            },
            "speaker": prev_segment["label"],
        })
    
    return new_segments

@with_imports("numpy")
def align_segments_with_transcript(
    new_segments: List[Dict[str, Any]], 
    transcript_chunks: List[TranscriptChunk],
    *, 
    dynamic_imports: Dict[str, Any] = {}
) -> List[DiarizedChunk]:
    """
    Alinea los segmentos de diarizaci√≥n con la transcripci√≥n.
    
    Args:
        new_segments: Segmentos de diarizaci√≥n
        transcript_chunks: Fragmentos de transcripci√≥n
        dynamic_imports: M√≥dulos importados din√°micamente
        
    Returns:
        List[DiarizedChunk]: Fragmentos con informaci√≥n de hablante
    """
    np = dynamic_imports["numpy"]
    segmented_preds: List[DiarizedChunk] = []
    
    if not new_segments or not transcript_chunks:
        return segmented_preds
    
    # Obtener timestamps finales de cada fragmento transcrito
    end_timestamps = np.array([
        chunk["timestamp"][-1] if chunk["timestamp"][-1] is not None 
        else sys.float_info.max for chunk in transcript_chunks
    ])
    
    # Alinear los timestamps de diarizaci√≥n y ASR
    for segment in new_segments:
        end_time: float = segment["segment"]["end"]
        # Encontrar el timestamp de ASR m√°s cercano
        upto_idx: int = np.argmin(np.abs(end_timestamps - end_time))
        
        # Agregar fragmentos con informaci√≥n de hablante
        for i in range(upto_idx + 1):
            chunk: TranscriptChunk = transcript_chunks[i]
            segmented_preds.append({
                "text": chunk["text"],
                "timestamp": chunk["timestamp"],
                "speaker": segment["speaker"]
            })
        
        # Recortar la transcripci√≥n y timestamps para el siguiente segmento
        transcript_chunks = transcript_chunks[upto_idx + 1:]
        end_timestamps = end_timestamps[upto_idx + 1:]
        
        if len(end_timestamps) == 0:
            break
    
    return segmented_preds

@log_time
@with_imports("torch", "pyannote.audio", "numpy")
def diarize_audio(
    config: TranscriptionConfig, 
    audio_data: AudioData, 
    transcript: TranscriptOutput,
    *, 
    dynamic_imports: Dict[str, Any] = {}
) -> List[DiarizedChunk]:
    """
    Realiza la diarizaci√≥n para identificar diferentes hablantes.
    
    Args:
        config: Configuraci√≥n de transcripci√≥n/diarizaci√≥n
        audio_data: Audio procesado para diarizaci√≥n
        transcript: Resultados de la transcripci√≥n
        dynamic_imports: M√≥dulos importados din√°micamente
        
    Returns:
        List[DiarizedChunk]: Transcripci√≥n con informaci√≥n de hablantes
    """
    # Si no hay token, omitir diarizaci√≥n
    if config.hf_token == "no_token":
        logger.info("Diarizaci√≥n omitida (no se proporcion√≥ token)")
        return []
    
    logger.info(f"Iniciando diarizaci√≥n con modelo {config.diarization_model}")
    
    def execute_diarization() -> Tuple[List[DiarizedChunk], List[Dict[str, Any]]]:
        # 1. Cargar pipeline
        diarization_pipeline = load_diarization_pipeline(
            config, dynamic_imports={"torch": dynamic_imports["torch"], 
                                    "pyannote.audio": dynamic_imports["pyannote.audio"]}
        )
        
        # 2. Ejecutar diarizaci√≥n
        diarization = diarization_pipeline(
            {"waveform": audio_data["torch_tensor"], "sample_rate": 16000},
            num_speakers=config.num_speakers,
            min_speakers=config.min_speakers,
            max_speakers=config.max_speakers,
        )
        
        # 3. Procesar segmentos
        new_segments: List[Dict[str, Any]] = process_diarization_segments(diarization)
        
        # 4. Alinear con transcripci√≥n
        transcript_chunks: List[TranscriptChunk] = transcript["chunks"].copy()
        segmented_preds: List[DiarizedChunk] = align_segments_with_transcript(
            new_segments, transcript_chunks, 
            dynamic_imports={"numpy": dynamic_imports["numpy"]}
        )
        
        return segmented_preds, new_segments
    
    segmented_preds, new_segments = with_progress_bar("Segmentando hablantes...", execute_diarization)
    
    num_speakers: int = len(set(segment["speaker"] for segment in new_segments)) if new_segments else 0
    logger.info(f"Diarizaci√≥n completa: {num_speakers} hablantes detectados")
    
    return segmented_preds

@log_time
def build_and_save_result(
    config: TranscriptionConfig, 
    transcript: TranscriptOutput, 
    speakers_transcript: List[DiarizedChunk]
) -> FinalResult:
    """
    Construye y guarda el resultado final.
    
    Args:
        config: Configuraci√≥n de transcripci√≥n
        transcript: Resultados de la transcripci√≥n
        speakers_transcript: Transcripci√≥n con info de hablantes
        
    Returns:
        FinalResult: Resultado final con toda la informaci√≥n
    """
    # Construir resultado final
    logger.info("Construyendo resultado final")
    result: FinalResult = {
        "speakers": speakers_transcript,
        "chunks": transcript["chunks"],
        "text": transcript["text"],
    }
    
    # Guardar resultado
    output_path: Union[str, Path] = config.transcript_path
    logger.info(f"Guardando resultado en {output_path}")
    with open(output_path, "w", encoding="utf8") as fp:
        json.dump(result, fp, ensure_ascii=False)
    
    logger.info(f"Voila!‚ú® Archivo guardado en: {output_path}")
    return result

@log_time
def main() -> FinalResult:
    """
    Funci√≥n principal que coordina todo el proceso.
    
    Returns:
        FinalResult: Resultado final del proceso
    """
    try:
        # 1. Procesar argumentos
        config: TranscriptionConfig = parse_arguments()
        
        # 2. Procesar audio - usa el decorador @with_imports
        audio_data: AudioData = process_audio(config.file_name)
        
        # 3. Transcribir audio - usa el decorador @with_imports
        transcript: TranscriptOutput = transcribe_audio(config, audio_data)
        
        # 4. Diarizar audio (opcional) - usa el decorador @with_imports
        speakers_transcript: List[DiarizedChunk] = diarize_audio(config, audio_data, transcript)
        
        # 5. Construir y guardar resultado
        result: FinalResult = build_and_save_result(config, transcript, speakers_transcript)
        
        return result
    except Exception as e:
        logger.error(f"Error durante la ejecuci√≥n: {str(e)}", exc_info=True)
        raise

if __name__ == "__main__":
    main()